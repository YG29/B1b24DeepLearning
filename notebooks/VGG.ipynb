{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## VGG-16"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2748a678e7ad77e5"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler, random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:20:41.178199Z",
     "start_time": "2025-01-25T18:20:41.175609Z"
    }
   },
   "id": "ceca54e19df3454a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS (Metal Performance Shaders) GPU is available.\")\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-25T18:20:46.709203Z",
     "start_time": "2025-01-25T18:20:46.685204Z"
    }
   },
   "id": "d1f4938778809925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) GPU is available.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T19:10:49.955057Z",
     "start_time": "2025-01-25T19:10:49.925625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define transform for train val and test\n",
    "image_transforms = {\n",
    "    # training with augmentation\n",
    "    'train':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]),\n",
    "    'val':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ]),\n",
    "    'test':\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "}\n",
    "\n",
    "fashion_full_train = torchvision.datasets.FashionMNIST('../data', train=True, download=True)\n",
    "fashion_test = torchvision.datasets.FashionMNIST('../data', train=False, transform=image_transforms['test'], download=True)\n"
   ],
   "id": "9d91a608f52eca02",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T19:10:50.964837Z",
     "start_time": "2025-01-25T19:10:50.963161Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(fashion_test))",
   "id": "2390cbc61a8cccb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T19:10:52.746827Z",
     "start_time": "2025-01-25T19:10:52.744177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_mapping = {\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot'\n",
    "}"
   ],
   "id": "6df9d82282aea072",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T19:10:53.869120Z",
     "start_time": "2025-01-25T19:10:53.861771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "fashion_train, fashion_val = random_split(fashion_full_train, [int(len(fashion_full_train)*0.8), int(len(fashion_full_train)*0.2)])\n",
    "\n",
    "fashion_train.dataset.transform = image_transforms['train']\n",
    "fashion_val.dataset.transform = image_transforms['val']\n"
   ],
   "id": "7b03367b04591d4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T19:12:06.490090Z",
     "start_time": "2025-01-25T19:12:06.485857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(fashion_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(fashion_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(fashion_test, batch_size=batch_size, shuffle=True)"
   ],
   "id": "354c8caae8735336",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:44:09.273671Z",
     "start_time": "2025-01-25T20:44:09.251654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get a single batch from the train_loader\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Check the structure of the images and labels\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ],
   "id": "afc42c05a600aab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([8, 3, 224, 224])\n",
      "Labels shape: torch.Size([8])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:13:32.869422Z",
     "start_time": "2025-01-25T20:10:49.082344Z"
    }
   },
   "cell_type": "code",
   "source": "model_vgg16 = models.vgg16(weights='DEFAULT')",
   "id": "5d63528dfd2f4951",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/ygao/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "8.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model_vgg16 \u001B[38;5;241m=\u001B[39m models\u001B[38;5;241m.\u001B[39mvgg16(weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDEFAULT\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/models/_utils.py:142\u001B[0m, in \u001B[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    135\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msequence_to_str(\u001B[38;5;28mtuple\u001B[39m(keyword_only_kwargs\u001B[38;5;241m.\u001B[39mkeys()),\u001B[38;5;250m \u001B[39mseparate_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand \u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as positional \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    137\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    139\u001B[0m     )\n\u001B[1;32m    140\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate(keyword_only_kwargs)\n\u001B[0;32m--> 142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/models/_utils.py:228\u001B[0m, in \u001B[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m kwargs[pretrained_param]\n\u001B[1;32m    226\u001B[0m     kwargs[weights_param] \u001B[38;5;241m=\u001B[39m default_weights_arg\n\u001B[0;32m--> 228\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/models/vgg.py:433\u001B[0m, in \u001B[0;36mvgg16\u001B[0;34m(weights, progress, **kwargs)\u001B[0m\n\u001B[1;32m    413\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001B[39;00m\n\u001B[1;32m    414\u001B[0m \n\u001B[1;32m    415\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;124;03m    :members:\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    431\u001B[0m weights \u001B[38;5;241m=\u001B[39m VGG16_Weights\u001B[38;5;241m.\u001B[39mverify(weights)\n\u001B[0;32m--> 433\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _vgg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m, weights, progress, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/models/vgg.py:105\u001B[0m, in \u001B[0;36m_vgg\u001B[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m model \u001B[38;5;241m=\u001B[39m VGG(make_layers(cfgs[cfg], batch_norm\u001B[38;5;241m=\u001B[39mbatch_norm), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 105\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(weights\u001B[38;5;241m.\u001B[39mget_state_dict(progress\u001B[38;5;241m=\u001B[39mprogress, check_hash\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/models/_api.py:90\u001B[0m, in \u001B[0;36mWeightsEnum.get_state_dict\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_state_dict\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Mapping[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m---> 90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_state_dict_from_url(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/hub.py:867\u001B[0m, in \u001B[0;36mload_state_dict_from_url\u001B[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001B[0m\n\u001B[1;32m    865\u001B[0m         r \u001B[38;5;241m=\u001B[39m HASH_REGEX\u001B[38;5;241m.\u001B[39msearch(filename)  \u001B[38;5;66;03m# r is Optional[Match[str]]\u001B[39;00m\n\u001B[1;32m    866\u001B[0m         hash_prefix \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m r \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m     download_url_to_file(url, cached_file, hash_prefix, progress\u001B[38;5;241m=\u001B[39mprogress)\n\u001B[1;32m    869\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_legacy_zip_format(cached_file):\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/hub.py:744\u001B[0m, in \u001B[0;36mdownload_url_to_file\u001B[0;34m(url, dst, hash_prefix, progress)\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(\n\u001B[1;32m    737\u001B[0m     total\u001B[38;5;241m=\u001B[39mfile_size,\n\u001B[1;32m    738\u001B[0m     disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m progress,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    741\u001B[0m     unit_divisor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m,\n\u001B[1;32m    742\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m         buffer \u001B[38;5;241m=\u001B[39m u\u001B[38;5;241m.\u001B[39mread(READ_DATA_CHUNK)\n\u001B[1;32m    745\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    746\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/http/client.py:473\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    472\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 473\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mread(amt)\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/socket.py:718\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 718\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    719\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    720\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/ssl.py:1314\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1310\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1311\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1312\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1313\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread(nbytes, buffer)\n\u001B[1;32m   1315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/anaconda3/envs/deeplearning/lib/python3.11/ssl.py:1166\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1166\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1168\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:04:42.633801Z",
     "start_time": "2025-01-25T20:04:42.621722Z"
    }
   },
   "cell_type": "code",
   "source": "model_vgg16",
   "id": "7192fa350aa1a72f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_vgg16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model_vgg16\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_vgg16' is not defined"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Freeze early layers\n",
    "for param in model_vgg16.parameters():\n",
    "    param.requires_grad = False"
   ],
   "id": "cf057ec80f801301"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_classes = 10\n",
    "n_inputs = model_vgg16.classifier[6].in_features\n",
    "model_vgg16.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, n_classes),\n",
    "    nn.LogSoftmax(dim=1))\n",
    "\n",
    "model_vgg16.classifier"
   ],
   "id": "28c2a88104be23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# summary(model_vgg16, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')",
   "id": "1ed890c9930b13af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_vgg16.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available() else\n",
    "    torch.device(\"cuda\") if torch.cuda.is_available() else\n",
    "    torch.device(\"cpu\")\n",
    ")\n",
    "model_vgg16 = model_vgg16.to(device)\n",
    "# model_vgg16.cuda()"
   ],
   "id": "c70689c10f56d85d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # model_vgg16.train()  # do I want to set the model to training? no\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_vgg16(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # validation loop\n",
    "    # model_vgg16.eval()  # guess no need for setting\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # gradient computation for validation\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_vgg16(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # average losses and accuracy\n",
    "    avg_training_loss = running_loss / len(train_loader)\n",
    "    avg_validation_loss = validation_loss / len(val_loader)\n",
    "    validation_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_training_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_validation_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {validation_accuracy:.2f}%\")\n",
    "\n",
    "print('Fine-tuning complete!')\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model_vgg16.state_dict(), 'finetuned_vgg16_mnist.pth')\n",
    "print('Model saved!')"
   ],
   "id": "cbd1af6b970258a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77e91f37e624d4d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(deeplearning_ipykernel)\n",
   "language": "python",
   "name": "deeplearning_ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
